# 架构设计：jz-flow

jz-flow 的核心在于提供高效的数据流功能，使数据能够在 DAG（有向无环图）中流动。每当数据流经一个节点时，该节点将对数据施加特定的影响，改变其形态。每个节点可以读取流入的数据，进行处理，并将处理后的数据输出到数据流中。借助 Kubernetes 强大的扩展和调度能力，本项目无需关注复杂的部署和调度问题，而是将重点放在图定义、数据协议设计以及节点实现上。

## 核心组件

1. **DAG 图**  
   DAG 图定义了以下内容：
   - 用户镜像
   - 命令
   - 副本数
   - 节点间的前序和后续关系

2. **数据协议**  
   数据协议保证数据能够安全、无损地从上游节点传递到下游节点，确保数据在传输过程中保持一致性。

3. **计算单元 (Compute Unit)**  
   计算单元用于对数据施加某种影响。在实际使用中，通过 Kubernetes 启动一个或多个计算节点，这些节点并行获取数据并处理数据。计算单元的设计重点是支持自定义逻辑，以适应不同的数据类型（如表格、文件、视频、模型等）。同时，为了增强复用性，计算单元应具备通用性，避免重复工作。

### 计算单元控制接口

计算单元提供以下控制接口：
   1. `status`: 获取计算单元的状态。
   2. `start`: 告知计算单元开始计算。
   3. `pause`: 暂停计算单元的运算。
   4. `restart`: 恢复暂停的计算单元。
   5. `stop`: 结束当前任务。

4. **元数据库 (MetaStore)**  
   每个 DAG 运行时都维护一个元数据库，用于记录每个节点的每个数据批次状态。数据批次经历 `Received`、`Processed` 和 `Clean` 状态。为了保证数据的安全性：
   - 在 `Received` 状态之前，上游节点不能删除数据。
   - 在 `Processed` 状态之前，当前节点必须保留数据。

   元数据库中的记录包括：
   1. `global`: 保存全局状态（图形态、图状态）。
   2. `node`: 记录每个节点的状态。
   3. `data batch`: 记录每个数据批次的状态。

5. **缓存数据 (TempFileStore)**  
   缓存数据用于暂时存储接收到的数据，有两种模式：
   - **持久化缓存**: 防止系统崩溃或 bug 导致的数据丢失，适用于文件系统、对象存储、消息队列等。
   - **内存缓存**: 提供快速数据访问，但抗风险能力较差。

## 计算单元运行模型

计算单元运行在 Kubernetes 的 Pod 中，每个 Pod 包含两个容器：数据容器和用户容器。

1. **数据容器**: 控制数据流，从前序节点接收数据并记录保存，将用户容器生成的数据发送给后续节点。数据容器还负责管理数据状态的流转，并为用户容器提供接口，以便用户容器从数据流中提取或写入数据。

2. **用户容器**: 允许用户自定义镜像，从特定位置读取数据，处理数据，并将结果写入给定的输出位置。

## 数据生命周期管理

通过数据标识控制数据的生命周期和可用性。将大数据集分为多个小批次（batch），并逐个发送以分片处理，便于通过扩展节点数量提升处理能力。

### 标签机制

- **Keep Data**: 数据传输后保存该数据。
- **Transparent Transfer**: 数据仅传输，不主动处理，但用户可以主动检索。

## 传输协议

传输协议采用 gRPC，因为 protobuf 协议紧凑且高效。

## 数据库

数据库采用 MongoDB，具备大容量吞吐能力、动态建表、数据结构灵活的特点。

## 云原生架构

所有组件部署在 Kubernetes 集群中，每个计算节点是一个独立的部署单元。借助 Kubernetes 的 storageclass 机制，为 DAG 中的节点快速生成缓存存储。

## 日志采集及运行状态监控

通过 Kubernetes 系统采集所有节点日志，汇总节点状态信息（如数据吞吐量、CPU、内存、磁盘、网络等），并在 UI 上综合展示。

## 待确认 (TBD)
